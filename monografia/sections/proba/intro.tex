\documentclass[../main.tex]{subfiles}

\begin{document}

\subsection{Introducción a la probabilidad}

\paragraph{} Una de las intenciones principales de la ciencia es poder modelar, razonar, y explicar al mundo real. Con este objetivo, se desea poder hablar matemáticamente sobre eventos no necesariamente certeros.

La teoría de Probabilidades comienza en los siglos XVI y XVII cuando Georlamo Cardano, Pierre de Fermat, y Blaise Pascal intentan analizar juegos de azar. Luego, durante los siguientes dos siglos varios matemáticos como Jacob Bernoulli y Abraham de Moivre la expandieron. Y en 1812 Pierre de Laplace introdujo la definición clásica de probabilidad.

Con estos inicios, la probabilidad originalmente sólo consideraba eventos discretos, y sus métodos eran más que nada combinatorios. Por ejemplo, para calcular la probabilidad de que tirando una moneda salga cara (\(H\)) calculamos la frecuencia de veces en las que sale entre todos los resultados posibles, que en este caso serían \(\{H, T\}\). Esto nos dejaría con que \(\frac{\abs{\{H\}}}{\abs{\{H, T\}}} = \frac{1}{2} = 0.5 = 50\%\) de las veces ocurre el evento ''sale cara''.

De forma formal, para eventos discretos como es el de la moneda, se define la probabilidad \(P\) de que suceda un evento \(E\) en un espacio \(S\) de posibilidades como:
\begin{gather*}
  P(E) = \frac{\abs{\{s \in S \text{ tal que ocurre } E \text{ en } s\}}}{\abs{S}}
\end{gather*}
Y al estar analizando frecuencias se tiene que \(0 \leq P(E) \leq 1\). Donde un 0 sería que el evento es imposible ya que no sucede en \(S\), un 1 sería que el evento siempre sucede, y un número en el medio implica que a veces sucede.

Por ejemplo, tomando un dado de seis caras el resultado es un entero entre 1 y 6, el espacio \(S = \{1, 2, 3, 4, 5, 6\}\). Así que al tirar un dado tenemos que:
\begin{itemize}
  \item \(P(\text{''sale un 4''}) = \dfrac{\abs{\{4\}}}{\abs{\{1, 2, 3, 4, 5, 6\}}} = \frac{1}{6}\). Similar para cualquiera de las seis caras.
  \item \(P(\text{''sale un 4''}) = \dfrac{\abs{\{4\}}}{\abs{\{1, 2, 3, 4, 5, 6\}}} = \dfrac{1}{6}\). Similar para cualquiera de las seis caras.
  \item \(P(\text{''no sale un 6''}) = \dfrac{\abs{\{1, 2, 3, 4, 5\}}}{\abs{\{1, 2, 3, 4, 5, 6\}}} = \dfrac{5}{6}\). Similar para cualquiera de las seis caras.
\item \(P(\text{''sale par''}) = \dfrac{\abs{\{2, 4, 6\}}}{\abs{\{1, 2, 3, 4, 5, 6\}}} = \dfrac{3}{6} = \dfrac{1}{2}\). Similar para que salga impar.
\end{itemize}

\paragraph{} En este texto, a menos que lo aclare, voy a estar hablando sobre probabilidades discretas. Donde el espacio de valores es finito o infinito numerable (\(\{0, 1\}\), \(\mathbb{N}\)). También existe la probabilidad continua, donde el espacio de valores es infinito no-numerable (\(\mathbb{R}\)), varias cosas que se valen para la probabilidad discreta no se valen en la continua pero tienen ideas análogas, sobre esto se habla en la sección \ref{seq:proba:continua}. % TODO: Texto? Monografía?

\subsubsection{Calculando probabilidades}
\paragraph{} Para calcular la probabilidad de un evento podemos o usar combinatoria o simular el proceso que genera al evento. Como ejemplo vamos a calcular la probabilidad de sacar tres cartas del mismo valor de un mazo de 48 cartas bien mezclado.

\paragraph{} El método combinatorio es el mismo método que vimos para los dados. Hay \(\binom{48}{3}\) formas de agarrar tres cartas cualquieras del mazo. Y como hay \(12\) posibles valores para las cartas y \(\binom{4}{3}\) formas de agarrar 3 palos de los cuatro palos posibles. Queda que hay \(12\binom{4}{3}\) casos en los que se cumple lo pedido. Por lo que la probabilidad del evento es:
\begin{gather*}
  \dfrac{12\binom{4}{3}}{\binom{48}{3}} = \dfrac{12 \cdot 4}{17296} = \dfrac{48}{17296} = \dfrac{3}{1081} \approx 0.0028 = 0.28\%
\end{gather*}

\paragraph{} En el otro método simulamos el proceso que genera al evento. Que en este caso consiste de tres pasos en los que levantamos una carta en cada uno. Donde queremos que cada paso mantenga el evento deseado.
\begin{enumerate}
  \item Agarrando la primera carta no importa cuál salga, porque no hay ninguna restricción.
  \item Agarrando la segunda de las 47 cartas restantes sólo nos sirven 3, por lo que hay \(\frac{3}{47}\) chances de que siga el evento.
  \item Y agarrando la tercera carta, similarmente, quedan 46 cartas y sólo nos sirven 2, por lo que hay \(\frac{2}{46}\) chances.
\end{enumerate}
Por lo que la probabilidad del evento es:
\begin{gather*}
  1 \cdot \dfrac{3}{47} \cdot \dfrac{2}{46} = \dfrac{3}{1081} \approx 0.0028 = 0.28\%
\end{gather*}

\subsubsection{Operaciones sobre eventos}

\paragraph{} Al definir los eventos como conjuntos, se los puede manipular con las operaciones de ellos. En específico hay cuatro operaciones principales:
\begin{itemize}
  \item El \textbf{complemento} de un evento \(\bar{E}\), que son los casos en los que no pasa el evento \(E\):
    \begin{gather*}
      P(\bar{E}) = 1 - P(E)
    \end{gather*}
  \item La \textbf{intersección} de dos eventos, que son los casos en los que pasan ambos a la vez: \(P(E_{1} \cap E_{2})\), cuya ecuación veremos a continuación.
  \item La \textbf{unión} de dos eventos, que son los casos en los que pasa uno o el otro (o ambos):
    \begin{gather*}
      P(E_{1} \cup E_{2}) = P(E_{1}) + P(E_{2}) - P(E_{1} \cap E_{2})
    \end{gather*}
    Restamos los casos en los que pasan ambos, ya que los estaríamos contando dos veces al sumarlos.
  \item Y la \textbf{probabilidad condicional} de que pase un evento \(A\) dado que pasa otro evento \(B\)
    \begin{align*}
      P(A | B) = \dfrac{P(A \cap B)}{P(B)} && P(B) \neq 0
    \end{align*}
    Que intuitivamente significa reducir el espacio \(S\) al sub-espacio \(S_{B} \subset S\) donde que sólo contiene los casos donde \(B\) sucede. \\
    Notar que no estaría bien definido en el caso que \(P(B) = 0\) ya que como no puede suceder \(B\) no puede suceder \(A\) dado \(B\).
\end{itemize}

Con esta última se puede derivar la probabilidad de la intersección de dos eventos como: 
\begin{gather*}
  P(A \cap B) = \begin{cases} P(A | B) \cdot P(B) & \text{si } P(B) \neq 0 \\ 0 & \text{en otro caso} \end{cases}
\end{gather*}

Cuando sucede que \(P(A|B) = P(A)\) y \(P(B|A) = P(B)\) se los define como eventos \textbf{independientes} entre sí, y se tiene que:
\begin{gather*}
  P(A \cap B) = P(A) \cdot P(B)
\end{gather*}

\end{document}

\subsection{Variables aleatorias}

\paragraph{} Un uso común de la probabilidad es al realizar experimentos donde se mide un evento aleatorio. En estos casos es útil pensar en una función que represente a los resultados posibles. A esta función la nombramos \textbf{variable aleatoria (v.a.)}. \textit{Variable} porque toma distintos valores y \textit{aleatoria} porque el valor observado no puede ser predicho antes de la realización del experimento, aunque sí se sabe cuales son sus posibles valores.

Dado que el valor de una v.a. es determinado por el resultado de un experimento, podemos asignarle probabilidades a los posibles valores de la variable.

Como ejemplo, al arrojar dos veces un dado bien equilibrado. El espacio asociado es:
\begin{gather*}
  S = \{(x_{1}, x_{2}) / x_{i} \in \{1, 2, 3, 4, 5, 6\}\}
\end{gather*}

Y para este experimento tenemos (entre otras) las siguientes v.a. asociadas:
\begin{itemize}
  \item \(X:\) ''número de caras pares''.
  \item \(Y\): ''máximo''.
  \item \(Z\): ''la suma''.
\end{itemize}

\paragraph{} Formalmente, dado un espacio \(S\) definimos una variable aleatoria \(X\) como una función tal que:
\begin{gather*}
  X : S \rightarrow \mathbb{R}
\end{gather*}

Con los ejemplos anteriores tendríamos:
\begin{align*}
  X((2, 5)) &= 1 & Y((2, 5)) &= 5 & Z((2, 5)) &= 7 \\
  X((1, 3)) &= 0 & Y((1, 3)) &= 3 & Z((1, 3)) &= 4 \\
  X((2, 2)) &= 2 & Y((2, 2)) &= 2 & Z((2, 2)) &= 4 \\
\end{align*}

\paragraph{} Ahora entonces definimos la \textbf{función de probabilidad puntual} de la v.a. \(X\) para todo \(x \in \mathbb{R}_{X}\), donde \(\mathbb{R}_{X}\) es el rango de \(X\) como:
\begin{gather*}
  P(X = x) = P(\{s \in S / X(s) = x\})
\end{gather*}

Que extiende la idea de probabilidad sobre eventos para que aplique sobre variables aleatorias donde el evento sería el caso en que \(X\) vale \(x\).

Es importante resaltar la siguiente propiedad derivada de que seguimos hablando sobre la frecuencia de eventos dentro de un espacio de posibilidades:
\begin{align*}
  \sum_{x \in \mathbb{R}_{X}}P(X = x) = 1
\end{align*}

\paragraph{} También definimos la \textbf{función de distribución acumulada} de una v.a. \(X\) como la probabilidad de que \(X\) sea menor o igual que un valor \(x\), formalmente:
\begin{align*}
  P(X \leq x) = \sum_{y \leq x, y \in \mathbb{R}_{X}}P(X = y) && \forall x \in \mathbb{R}
\end{align*}

\subsubsection{Distribuciones}

\end{document}
